---
title: "游꺐 Object Detection med Machine Learning og ESP32Cam"
description: "I dag vil jeg dykke ned i hvordan man med machine learning og et ~50 kroners kamera kan identificere specifikke objekter, og hvordan jeg implementerer det."
author: "mikkelrask"
date: 2024-06-26
tags:
  - machine learning
  - embedded programming
  - tinkering
  - hardware
---

![](./not-a-hotdog.jpg)

## Object Detection med ML og ESP32CAM

I dette indl칝g g친r jeg igennem hvordan jeg har tr칝net en machine learning (ML) model p친 147 billeder af AI genererede hus-skader og efterf칮lgende har flashet modellen p친 en microcontroller, til at identificere hus-skader, med et kamera, til under 50 kroner.

![](ytho.png)

## Jo ser du...

_"Jamen hvorfor vil du dog d칠t, Mikkel?"_, h칮rer jeg dig m친ske sp칮rge..
Mit form친l er ret simpelt. Hvis du evt. f칮lger min [instagram](https://instagram.com/mikkelraskdk) ved du m친ske at jeg har en (m친ske flere游뱡) [hus-skader](https://da.wikipedia.org/wiki/Husskade), der af og til bes칮ger mig n친r jeg har 친bne vinduer, som jeg godt kunne t칝nke mig at fange et billede eller video af. Den (eller de) er dog ret sky til trods for at jeg er ret s친 overbevist om at en var helt inde i k칮kkenet forleden og rode lidt rund i nogle madrester, s친 den er ofte v칝k igen f칮r jeg har f친et sn칮vligt min mobil op af lommen eller et kamera klar.

S친 jeg har selvf칮lgeligt sat et kamera op, der streamer live men for at slippe for at blot sidde og holde 칮je med en livestream af mit k칮kkenvindue, tage tusindevis af billeder, som jeg efterf칮lgende skal kigge igennem efter _eventuelle_ fugle, eller endda optage alt der foreg친r med samme form친l, vil jeg med min ML-model og det [billige ~50 kroners ESP32CAM board](https://www.aliexpress.com/item/1005002519844156.html?spm=a2g0o.productlist.main.17.6d2aaYJVaYJVCt&algo_pvid=526b4cb9-843d-4b01-b1c2-38a8f55fd70b&algo_exp_id=526b4cb9-843d-4b01-b1c2-38a8f55fd70b-8&pdp_npi=4%40dis%21DKK%2164.58%2142.59%21%21%219.28%216.12%21%402103080917193008728458704e7f66%2112000021135848099%21sea%21DK%210%21AB&curPageLogUid=SCp4ogTQTeQg&utparam-url=scene%3Asearch%7Cquery_from%3A) peget imod mit k칮kkenvindue, kunne identificere n친r der er en fugl _in frame_, og kun d칠r tage og gemme billeder eller optage videoen.

## Hvad g친r jeg igennem

Jeg runder f칮lgende emner, som du kan navigere direkte til via linksne i h칮jre side, hvis du er s친dan lidt _TL;DR_-agtig, ligesom jeg i bunden af indl칝gget har listet de referencer jeg har brugt for at l칝re mere om emnet.

Her vil jeg is칝r n칝vne Dronebot Workshops video omkring emnet, som n칝rmest er en videoudgave af dette indl칝g, hvis det mere er noget for dig.

- Hvad er Machine Learning?
  - Forskellige typer Machine Learning
  - Supervised Learning
- Tr칝ning af modellen
  - Forberedelse af tr칝ningsdata
  - Tr칝ning af ML-modeller med Edge Impulse
  - Eksporter modellen
- Flash model-firmware p친 ESP32Cam
- Resultatet

## Hvad er Machine Learning

Machine Learning er hvad vi havde f칮r de AI's der dominerer markedet i dag - faktisk er mange services og apps der reklamerer med at v칝re "AI" ofte blot machine learning i forkl칝dning.
![original strip af [sandserif](https://instagram.com/sandserif)](strip-by-sandserif.webp)
Machine learning er reelt set et _subset_ af AI, m친ske omvendt - men det fungerer ved, at man tr칝ner en model med specifik information, hvorefter den kan genkende de m칮nstre der udg칮r informationen, og selv derfra enten oprette mere data, eller identifere om det givne data er hvad den er tr칝net p친 eller ej. Machine learning er meget omkring sansynlighedsberegning, og man f친r derfor en s친kaldt _confidence score_, da, med mindre d칠t modellen skal v칝re i stand til at identificere vitterligt var repr칝senteret 1:1 i modellens tr칝ningsdata, aldrig er med 100% sikkerhed, det som modellen ser nu.
Det er lidt sat p친 en spids, men er s친dan _the gist of it_..

### Forskellige typer Machine Learning

Der er forskellige typer af machine learning, alle med hvert sit form친l:

- **Supervised Learning:** Modellen tr칝nes med data, der allerede har etiketter. For eksempel, billeder med etiketten "kat" eller "hund".
- **Unsupervised Learning:** Modellen finder selv m칮nstre i data uden etiketter. For eksempel, grupperer billeder baseret p친 lignende tr칝k uden at vide, hvad de forestiller.
- **Reinforcement Learning:** Modellen l칝rer gennem pr칮ving og fejl, og f친r "bel칮nninger" for at udf칮re opgaver korrekt.

### Supervised Learning

Vi tager udgangspunkt i _supervised learning_, hvor man tr칝ner sin ML model med en god h친ndfuld billeder af sine katte eller hvad end man vil identificere, og modellen vil herefter med ret stor pr칝cission - eller _confidence_, v칝re i stand til at identifere fx en kat fra en hund, eller en kat fra et menneske - hagen ved det, er dog at modellen ikke har kontekst for hvad hverken en hund eller en person er, s친 det er "_kat_" eller "_ikke en kat_".

![](bird-shazam.png)

Det kan alts친 _ikke_ identificere om der er tale om en norsk skovkat, eller en baby tiger, men blot om data passer p친 d칠t den har l칝rt at kende som en kat. Man _kan_ tr칝ne en supervised learning modeller til at b친de kende mennesker, hunde og katte, det er blot et sp칮rgsm친l om mere tr칝ning, og mere tr칝ningsdata, men til mit form친l, skal jeg blot kunne se, om der er en fugl i mit vidnue.

Kommer der mennesker flyvende ind af vinduet, har jeg andre problemer.
Har du set serien [Silicon Valley](https://thetvdb.com/series/silicon-valley), t칝nker du m친ske allerede her p친 [Jian Yang's "Not a Hotdog"](https://www.youtube.com/watch?v=vIci3C4JkL0)-app, hvilket er pr칝cist hvad vi er ude i, men hvor det dog i den virkelige verden fx. bruges til alt fra at screene MRI/CAT scanninger for eventuelle sygdomme, til at identificere trends i reel data, som aktiemarked-trends, _fraud detection_, selvk칮rende biler o.l. Med andre ord kan man med ML automatisere og forbedre mange opgaver, der tidligere kr칝vede menneskelig intelligens og/eller interaktion.

**Fun fact:** [Not a Hotdog](https://play.google.com/store/apps/details?id=com.codylab.seefood&hl=da) er lavet til en rigtig applikation, der kan findes p친 Playstore.

Med d칠t af vejen, _'sgo'then_!

## Tr칝ning af modellen

Som n칝vnt, skal man ved _supervised learning_ tr칝ne sin model ved at fodre den med data af d칠t man gerne vil identificere. B친de kvaliteten af ens data og hvor meget man tr칝ner den med, afg칮r selvf칮lgelig pr칝cissionen og kvaliteten af hvad modellen bliver i stand til - dens confidence.

Min umiddelbare f칮rste tanke var at lave en slags web-scraper med python, der blot ville s칮ge efter billeder af skader eller fugle s친dan lidt mere generelt, men jeg t칝nkte at eftersom vi ikke kan, eller beh칮ves at skelne imellem specifikke fulge arter, at det ville v칝re liges친 sjovt at tr칝ne modellen, med billeder genereret af husskader i stedet. S친 jeg har selvf칮lgeligt 친bnet mit trofaste [ComfyUI](https://github.com/comfyanonymous/ComfyUI), hvor jeg efter lidt _trial and error_ endte med AI modellen [dreamshaperXL](https://civitai.com/models/112902/dreamshaper-xl), som fik f칮lgende prompt:

```text
(eurasian:1.3) magpie, one bird, animal focus, neutral background, pica pica, corvidae, high quality, detailed,
```

Ikke at jeg tror det har den store indflydelse, som sagt skal vi ikke skelne imellem fuglesorter, men jeg m친tte dog understrege at det skulle v칝re _eurasion magpie_ og ikke den australske magpie, som er kendt for at v칝re ekstrem territoriale og er frygtet i store dele af australien for at [angribe mennesker](https://www.youtube.com/watch?v=S_aZj3OvIb4), hvis de eks. har fugleunger i n칝rheden. Som du ser er _eurasian_ i parantes, efterfulgt af et kolon og har 1.3 som v칝rdi efterf칮lgende, som g칮r at eurasian v칝gtes h칮jere i prompten end de 칮vrige ord. Jeg endte med en _execution time_ p친 ~20 sek per billede, og her er 칠t af resultaterne.
![](thismagpiedoesnotexist_00109_.png)

Mit simple workflow kan du downloade via skaden overfor, og blot tr칝kke-og-slippe det ind i ComfyUI hvis du selv skulle have lyst til/brug for at selv generere billeder af fugle, for _whatever reason_ - det er et _meget_ basic setup, der kan generere det meste, og er 100% prompt baseret.
![](comfyui.png)

### Forberedelse af tr칝ningsbilleder

Jeg endte med at kun generere ~150 billeder, da, som [denne artikel](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/) fra [Machine Learning Mastery](https://machinelearningmastery.com/) gennemg친r, virker billeder stadig fint til at tr칝ne ML modeller, hvis man flipper dem p친 x-aksen, justerer motivet op/ned, h칮jre eller venstre, eller hvis man _skew_'er det den ene eller anden vej. Det kaldes _data augmentation_ i machine learning verdnen, og er noget vi klarer i n칝ste trin under selve tr칝ningen!
Note: Jeg har i 칮vrigt gjort alle billederne frit tilg칝ngeligt b친de p친 projektets [Github-repo](https://github.com/mikkelrask/thismagpiedoesnotexist/tree/main/public/images), men ogs친 p친 [ThisMagpieDoesNotExist.pages.dev](https://thismagpiedoesnotexist.pages.dev), som er ment som et sjovt modstykke til [ThisPersonDoesNotExist.com](https://thispersondoesnotexist.com) - et API der supplerer billeder af mennesker der ikke findes.

### Tr칝n med Edge Impulse

Til at tr칝ne selve modellen bruger jeg dog en service der hedder [Edge Impulse](https://edgeimpulse.com) som er gratis at benytte til ikke-commercielle form친l, og da jeg ingen planer har om at s칝lge min _maggie-detector_ var det et nemt valg.

Her uploader man sit data (som kan v칝re alt muligt slags data, lyd, billeder, tekst, bin칝rt, etc), og som sk칝rmbilledet herunder viser, er det s친ledes s친 ved _object detection_, et sp칮rgsm친l om at lave en s친kaldt _bounding box_ omkring sit motiv, og give det et label, for hvert foto man tr칝ner modellen med.

Har man en enterprise/betalt konto hos dem, kan man bruge deres [YOLO-V5 AI baserede labeller](https://github.com/edgeimpulse/yolov5) til at klare det her ellers _liiidt_ kedelige trin. Jeg _raw-dogg'er_ den jo bare som altid, og m친 gl칝de mig ved, at det ikke vare 1000 billeder jeg besluttede at generere til at tr칝ne med.
![Image labelling med Edge Impulse](labelling.png)
Og uden det skal v칝re en guide i at bruge Edge Impulse, har jeg her et par ting, jeg vil notere ift at bruge dem som service.

Under **Impulse Design > Images** satte jeg h칮jde og bredde til 96px, og valgte at croppe p친 billedets l칝ngste led - da mine var kvadratiske fra start vil der ingen besk칝ring ske. Jeg valgte ogs친 at modellen skal tr칝nes p친 en _grayscale_ udgave af billederne, frem for RGB, for at spare lidt p친 ESP'ens begr칝nsede _computing power_.

Efter jeg klikkede **Save parameters**, blev jeg sendt videre til Generate Features fanen, hvor jeg blot klikkede p친 **Generate Features**-knappen.

S친 kan man g친 til **Impulse Design > Object Detection**, og det er her, at man v칝lger om man 칮nsker at tr칝ne modellen med _data augmentation_, der som tidligere n칝vnt modulerer dit tr칝ningsdata og herved giver yderligere data til at tr칝ne modellen ud fra.

For mig, var det tjekket til fra start, ligesom jeg ogs친 lod de 칮vrige parametre v칝re som default, med undtagelse af modellen - som jeg forst친r det, er _FOMO (Faster Objects, More Objects) MobileNetV2.0.1_ den eneste der underst칮ttes af ESP32, s친 det var den jeg valgte. Og s친 til sidst klikkede jeg p친 _Start Training_, og ventede i et par minutter.
![](data-augmentation.png)
Det gav mig en _F1 score_ p친 **98.1%**, hvilket skulle v칝re ret fornuftigt. (游뱡)

Jeg m친 indr칮mme, at jeg ikke er helt med p친 hvad v칝rdien er, men efter en chat med Gippety, fik jeg af vide at det udregnes ud fra pr칝cision og _recall_ v칝rdier, og udregnes som set herunder.

<p style="text-align:center;">

$$ \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}} $$

$$ \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}} $$

F1 Scoren er s친 det harmoniske gennemsnit af pr칝cision og recall:

$$ F1 = 2 \times \left( \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} \right) $$

</p>  

S친eh.. d칠t er ca. alt jeg kan forklare om F1 scoren... Uden at v칝re noget matematik-geni, vil jeg tro, at det betyder at mit ESP32CAM ville kunne identificere hus-skader der beslutter at titte hovedet ind af mit vindue med _op til_ 98.1% _confidence_ .

Og set i bagklogskabens lys, kunne jeg nok, nu hvor jeg selv genererede alle tr칝ningsbillederne, have gjort det uden baggrund, og potentielt f친et en h칮jere score.

### Eksporter modellen

Og det sidste vi skal klare p친 Edge Impulse er at eksportere vores nytr칝nede model. Edge Impulse kan levere din nytr칝nede ML model klargjort til adskille forskellige frameworks/typer af enheder, og jeg har valgt at benytte Espressif metode, da det er firmaet bag ESP32 enheder.

- G친 til **Deployment** fra menuen i venstre side, og p친 Deployment-siden klik i feltet _Search Deployment Options_, og s칮g p친 _Espressif ESP EYE (ESP32)_, og v칝lg den fra listen.
- Klik herefter p친 **Build**-knappen i bunden - det vil oprette og efterf칮lgende downloade et zip bibliotek til dig, med din firmware lige til at flashe!

NB! Du kan ogs친 v칝lge at hente det som et Arduino bibliotek hvis du er mere komfortabel med Arduino og deres IDE, men i s친 tilf칝lde, m친 du v칝re komfortabel nok, til at klare den solo herfra.  
![](onyaown.png)

## Flash firmware.bin

 skal vi en tur i terminalen - tilslut dit ESP32CAM via USB og naviger til din downloads mappe, eller hvor du gemte zip biblioteket. Pak filen ud, eks med unzip kommandoen:

```sh
unzip mrrask-project-1-esp32.zip
```

Dit bibliotek hedder selvf칮lgeligt **_dit_-brugernavn-og-projektnavn-esp32.zip** eller lign. og ikke det samme som mit.
Og Edge impulse leverer simpelthen et script, der mere eller mindrer klarer resten af arbjde for dig - denne kan k칮res ved at navigere til mappen der blev pakket ud, g칮re scriptet eksekverbar, hvorefter du blot k칮rer scriptet.

Alt hvad scriptet g칮r, er at via `pip` s칮rge for at hhv. `pyserial` og `esptool` er installeret, og bruger s친 efterf칮lgende `esptool` til at flashe firmwaren p친 din tilsluttede MCU. Mit output s친 s친ledes ud.  
![](1719343209.png)

S친 vi g칮r det manuelt her, for at understrege hvor lidt det drejer sig om, og fordi - man jo aldrig bare skal k칮rer shell scripts downloaded fra internettet, uden at f칮rst vide pr칝cist hvad de g칮r.
F칮rst skal du finde ud af hvilken COM port din ESP32 bruger, n친r den bliver tilsluttet - dette kan g칮res ved at k칮re `ls` p친 din `/dev/` mappe f칮r du tilslutter din ESP32, og sammenligne resultatet med hvad output du f친r af samme kommando efter du har tilsluttet den.

```sh
ls /dev > dev-before
# tilslut ESP32
ls /dev > dev-after
# Sammenlign de to resultater
diff dev-before dev-after
```

Ouputtet har potentielt adskillige diffs, men skulle gerne vise dig 칠n der hedder noget a la `ttyUSB0` - i s친 tilf칝lde er porten du skal kaste dit firmware efter `/dev/ttyUSB0`

```sh
python -m pip install esptool pyserial
esptool.py --chip esp32 --p /dev/ttyUSB0 -b 460800 write_flash 0x0 mrrask-project-1-esp32/firmware.bin
```

Her skal `mrrask-project-1-esp32/firmware.bin` selvf칮lgeligt udskiftes med mappenavnet du pakkede ud, eller blot `cd` ind i mappen forinden, og blot skrive `firmware.bin` i stedet.

N친r `esptool` er f칝rdig med at flashe, er vi i m친l. Herfra skal man blot overv친ge sin serial port (`/dev/ttsUSB0`), og kan med fx python nemt herfra automatisere hvad man 칮nsker der skal ske, n친r *insert-your-desired-detectable-thing* er er _in-frame_ - det er et helt indl칝g for sig, som det kan v칝re jeg vender tilbage til p친 et senere tidspunkt.

Du kan bl.a g칮res via Arduino IDE, eller igennem edge-impulse-cli v칝rkt칮j:
```sh
npm i -g edge-impulse-cli
edge-impulse-daemon
``` 
Dette kr칝ver at du har Node installeret i version 20 eller derover.

## Konklussion - how did it go?

Det kr칝ver lidt at der er en skade foran mit kamera, f칮r jeg med garanti kan sige at det virker, og evt. hvor godt. Og i skrivende stund har der, af hvad jeg ved, ikke v칝ret en henne og sidde i mit vindue. Hvis der har, virker det i hvert fald ikke s칝rlig godt.

Men heldigvis, s친 kan man direkte p친 Edge Impulse's hjemmeside teste sin model, med stillbilleder, og deres anbefaling er en 80/20 fordeling imellem tr칝nings- og test-materiale.

S친 denne gang, gik jeg faktisk online, og scrabede de ~30 fotos de anbefalede, til mine 150 tr칝ningsbilleder, netop s친 jeg kunne f친 den helt rigtige husskade, og ikke en eller anden [will-smith-spaghetti-spisende blanding](https://www.youtube.com/watch?v=XQr4Xklqzw8) af den australske og eurasiske skade, ligesom jeg ogs친 var nysgerrig om, hvad den ville sige til, at jeg smed et par ind, hvor skaden kommer flyvende - noget som modellen _ikke_ er tr칝net til at identificere.

Resultatet var **68.7% korrekte**, hvilket jo p친 ingen m친de er overv칝ldende, men der var stadig nogle af dem jeg ikke havde regnet med, at den ville v칝re i stand til at klassificere, som faktisk lykkedes, og som allerede n칝vnt var modellen ikke tr칝net p친 flyvende skader, hvilket i sig selv udgjorde 10% af test materialet _(3/30)_.

S친 jeg er ikke utilfreds med resultatet, og jeg var direkte p친 siden i stand til at tilf칮je de billeder der _ikke_ kunne identificeres til at nu v칝re med i tr칝ningsmaterialet, s친ledes at jeg kan gentr칝ne modellen, flashe p친 ny, med forh친bentlig endnu bedre resultater.
![](rede.png)
![](double-troube.png)  

Men det var faktisk d칠t der skulle til, for at tr칝ne en machine learning model til at kunne genkende en fugl - s친dan 50/50-ish, i hvert fald.
Er du n친et hertil, s친 vil jeg sige tak for at tage med p친 min lille _machine learning rejse_, det var et sjovt lille projekt. Jeg opdaterer selvf칮lgeligt min model og indl칝gget igen, hvis/n친r der er en husskade i kassen/k칮kkenet.

## Dokumentation

Her er ressourcerne jeg har brugt for at komme i m친l, hvor du kan l칝re meget mere om de forskellige elementer:  

**Edge Impulse:** 
- [edgeimpulse.com](https://edgeimpulse.com)
- [Officiel Dokumentation](https://docs.edgeimpulse.com/docs)
- [Edge Impulse for beginners (Officiel dokumentation)](https://docs.edgeimpulse.com/docs/readme/for-beginners)
- [Edge Impulse embedded learning (Officiel YT-video)](https://www.youtube.com/watch?v=dY3OSiJyne0)  
- [Dronebot Workshop tutorial](https://www.youtube.com/watch?v=HDRvZ_BYd08&t=1034s&pp=ygUNZWRnZSBpbXB1bHNlIA%3D%3D)  

**ESPTOOL:**
- [Officiel dokumentation](https://docs.espressif.com/projects/esptool/en/latest/esp32/)  
- [esptool p친 Pypi.org](https://pypi.org/project/esptool)  
- [espressif esptool github repo](https://github.com/espressif/esptool)

**Maggie Detecter**:
- [Github repo](#)
- [This Magpie Does Not Exist](https://thismagpiedoesnotexist.pages.dev)
